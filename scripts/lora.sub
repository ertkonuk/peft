#!/bin/bash
#SBATCH -A llmservice_modelalignment_ptune
#SBATCH -p batch_singlenode,batch_block1,batch_block3,batch_block2,batch_block4 # luna / backfill / interactive
#SBATCH -N 1                   # number of nodes
#SBATCH -t 4:00:00              # wall time  (4 for luna, 8 for backfill, 2 for interactive)
#SBATCH -J llmservice_modelalignment_ptune:peft.lora.llama-2-7b-hf-boolq  # job name (<< CHANGE ! >>)
#SBATCH --ntasks-per-node=8     # n tasks per machine (one task per gpu) <required>
#SBATCH --gpus-per-node=8     # n gpus per machine <required>
#SBATCH --gres=gpu:8            # Specify the number of GPUs even if exclusive. Needed for pytorch
#SBATCH  --mail-type=BEGIN,END,FAIL

source containers.sh
export HYDRA_FULL_ERROR=1

# training params
model_name="meta-llama/Llama-2-7b-hf"
task=$1
gbs=$2
mbs=$3
lr=$4
val_check=10
max_steps=2000
save_steps=10
max_seq_len=2048
adapter_dim=32

# force crashing on nccl issues like hanging broadcast
export NCCL_ASYNC_ERROR_HANDLING=1

proj_name="${task}_lora"
exp_name=`echo $SLURM_JOB_NAME | cut -d':' -f2-`
DATE=$(date "+%Y-%m-%d_%H-%M-%S")
DOCKER_EXP_DIR="/experiments/${proj_name}/${exp_name}/${DATE}"
EXP_DIR="$PROJHOME/${DOCKER_EXP_DIR}"
echo $proj_name $exp_name $EXP_DIR
mkdir -p $EXP_DIR

GPUS_PER_NODE=${SLURM_NTASKS_PER_NODE}
NNODES=${SLURM_JOB_NUM_NODES}
NUM_PROCESSES=$(expr $NNODES \* $GPUS_PER_NODE)

export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=12340
export WORLD_SIZE=${NUM_PROCESSES}
#grad_accum_steps=$(expr $gbs \/ $mbs) $(expr $gbs \/ $(expr $mbs \* $NUM_PROCESSES))
grad_accum_steps=4
# We will need this for 70B model training
#export LAUNCHER="accelerate launch \
#    --mixed_precision 'bf16' \
#    --main_process_ip $MASTER_ADDR \
#    --main_process_port $MASTER_PORT \
#    --machine_rank \$SLURM_PROCID \
#    --num_processes $NUM_PROCESSES \
#    --num_machines $NNODES \
#    "

#export LAUNCHER="torchrun \
#	--nproc_per_node=${SLURM_NTASKS_PER_NODE} \
#	--nnodes=${SLURM_JOB_NUM_NODES} \
#	--rdzv_backend=c10d \
#	--rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
#	"
export LAUNCHER="python"

read -r -d '' cmd <<EOF
echo "*******STARTING********" \
&& huggingface-cli login --token ${HF_TOKEN} \
&& export WANDB_API_KEY=${WANDB} \
&& cd /code/peft/scripts \
&& echo "Starting training" \
&& ${LAUNCHER} train.py \
--model_name=${model_name} \
--train_ds="/datasets/${task}/train.jsonl" \
--validation_ds="/datasets/${task}/validation.jsonl" \
--cache_dir=/hub \
--learning_rate=${lr} \
--lr_scheduler_type="cosine" \
--warmup_steps=50 \
--max_seq_len=${max_seq_len} \
--max_steps=${max_steps} \
--logging_steps=1 \
--eval_steps=${val_check} \
--save_steps=${save_steps} \
--bf16=True \
--packing=False \
--output_dir=${DOCKER_EXP_DIR}/training \
--per_device_train_batch_size ${mbs} \
--gradient_accumulation_steps ${grad_accum_steps} \
--use_peft_lora True \
--lora_r ${adapter_dim} \
--lora_alpha 1 \
--lora_dropout 0. \
--lora_qkv \
--use_4bit_quantization False \
--use_nested_quant False \
--bnb_4bit_compute_dtype "bfloat16" \
--use_flash_attn True \
--optim adamw_torch_fused
EOF

echo $cmd

srun -o ${EXP_DIR}/slurm-peft-%j-%n.out -e ${EXP_DIR}/slurm-peft-%j-%n.err --no-container-mount-home --container-image="$CONTAINER" $MOUNTS bash -c "${cmd}"
set +x
